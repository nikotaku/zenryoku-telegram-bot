"""
ã‚¨ã‚¹ãŸã¾ (estama.jp) ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆ â€” Ajax API ãƒ™ãƒ¼ã‚¹ã®æƒ…å ±å–å¾—
"""

import os
import re
import logging
import requests
from bs4 import BeautifulSoup

logger = logging.getLogger(__name__)

ESTAMA_LOGIN_ID = os.environ.get("ESTAMA_LOGIN_ID", "zr.sendai@gmail.com")
ESTAMA_PASSWORD = os.environ.get("ESTAMA_PASSWORD", "Zenryoku1209")

BASE_URL = "https://estama.jp"
ADMIN_URL = f"{BASE_URL}/admin"


class EstamaClient:
    """ã‚¨ã‚¹ãŸã¾ç®¡ç†ç”»é¢ã®ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆ"""

    def __init__(self):
        self.session = requests.Session()
        self.session.headers.update({
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
        })
        self._logged_in = False
        self._csrf_token = ""

    def _get_csrf_token(self, url: str) -> str:
        """ãƒšãƒ¼ã‚¸ã‹ã‚‰CSRFãƒˆãƒ¼ã‚¯ãƒ³ã‚’å–å¾—"""
        try:
            resp = self.session.get(url, timeout=15)
            soup = BeautifulSoup(resp.text, "html.parser")
            csrf_input = soup.find("input", {"id": "csrf_footer"})
            if csrf_input:
                return csrf_input["value"]
        except Exception:
            pass
        return ""

    def login(self) -> bool:
        """ã‚¨ã‚¹ãŸã¾ã«Ajaxãƒ­ã‚°ã‚¤ãƒ³"""
        try:
            # ãƒ­ã‚°ã‚¤ãƒ³ãƒšãƒ¼ã‚¸ã‹ã‚‰CSRFãƒˆãƒ¼ã‚¯ãƒ³å–å¾—
            self._csrf_token = self._get_csrf_token(f"{BASE_URL}/login/")
            if not self._csrf_token:
                logger.error("CSRFãƒˆãƒ¼ã‚¯ãƒ³å–å¾—å¤±æ•—")
                return False

            # Ajaxå½¢å¼ã§ãƒ­ã‚°ã‚¤ãƒ³ (jQuery serializeArrayäº’æ›)
            resp = self.session.post(
                f"{BASE_URL}/post/login_shop",
                data={
                    "str[0][name]": "mail",
                    "str[0][value]": ESTAMA_LOGIN_ID,
                    "str[1][name]": "password",
                    "str[1][value]": ESTAMA_PASSWORD,
                    "str[2][name]": "r",
                    "str[2][value]": "",
                    "ctk": self._csrf_token,
                },
                timeout=15,
                headers={
                    "Referer": f"{BASE_URL}/login/",
                    "X-Requested-With": "XMLHttpRequest",
                },
            )

            if resp.status_code == 200:
                try:
                    data = resp.json()
                    if data[0] == "OK" or data[0] == "REDIRECT_OK":
                        self._logged_in = True
                        logger.info("ã‚¨ã‚¹ãŸã¾ã«ãƒ­ã‚°ã‚¤ãƒ³æˆåŠŸ")
                        return True
                    else:
                        logger.error(f"ã‚¨ã‚¹ãŸã¾ãƒ­ã‚°ã‚¤ãƒ³å¤±æ•—: {data}")
                        return False
                except Exception:
                    pass

            logger.error(f"ã‚¨ã‚¹ãŸã¾ãƒ­ã‚°ã‚¤ãƒ³å¤±æ•—: {resp.status_code}")
            return False

        except Exception as e:
            logger.error(f"ã‚¨ã‚¹ãŸã¾ãƒ­ã‚°ã‚¤ãƒ³ã‚¨ãƒ©ãƒ¼: {e}")
            return False

    def _ensure_login(self) -> bool:
        if not self._logged_in:
            return self.login()
        # ã‚»ãƒƒã‚·ãƒ§ãƒ³åˆ‡ã‚Œãƒã‚§ãƒƒã‚¯
        try:
            resp = self.session.get(f"{ADMIN_URL}/", timeout=10, allow_redirects=False)
            if resp.status_code in (301, 302) and "/login" in resp.headers.get("Location", ""):
                self._logged_in = False
                return self.login()
        except Exception:
            pass
        return True

    def get_dashboard(self) -> dict:
        """ç®¡ç†ç”»é¢ãƒˆãƒƒãƒ—ã®æƒ…å ±ã‚’å–å¾—"""
        if not self._ensure_login():
            return {"error": "ãƒ­ã‚°ã‚¤ãƒ³ã«å¤±æ•—ã—ã¾ã—ãŸ"}

        try:
            resp = self.session.get(f"{ADMIN_URL}/", timeout=15)
            soup = BeautifulSoup(resp.text, "html.parser")
            text = soup.get_text(separator="\n", strip=True)

            result = {
                "shop_name": "",
                "shop_number": "",
                "plan": "",
                "contract_period": "",
                "points": "",
                "notifications": [],
                "menu_items": [],
            }

            lines = text.split("\n")
            for i, line in enumerate(lines):
                line = line.strip()
                if "å…¨åŠ›ã‚¨ã‚¹ãƒ†" in line and "ä»™å°" in line:
                    result["shop_name"] = line
                elif "åº—èˆ—ç•ªå·" in line:
                    result["shop_number"] = line
                elif "ã”å¥‘ç´„æœŸé–“" in line:
                    result["contract_period"] = line
                elif "ãƒ—ãƒ©ãƒ³" in line and ("ãƒ—ãƒ©ãƒãƒŠ" in line or "ã‚´ãƒ¼ãƒ«ãƒ‰" in line or "ã‚·ãƒ«ãƒãƒ¼" in line):
                    result["plan"] = line
                elif "ãƒã‚¤ãƒ³ãƒˆ" in line and i > 0:
                    # ãƒã‚¤ãƒ³ãƒˆæ•°ã‚’å–å¾—
                    for j in range(max(0, i-2), min(len(lines), i+2)):
                        if lines[j].strip().isdigit():
                            result["points"] = lines[j].strip()
                            break

            # äºˆç´„é€šçŸ¥ã‚’æ¢ã™
            links = soup.find_all("a")
            for link in links:
                href = link.get("href", "")
                text_content = link.get_text(strip=True)
                if "äºˆç´„" in text_content and href:
                    result["notifications"].append(text_content)

            return result

        except Exception as e:
            logger.error(f"ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
            return {"error": str(e)}

    def get_guidance_status(self) -> dict:
        """ã”æ¡ˆå†…çŠ¶æ³ã‚’å–å¾—"""
        if not self._ensure_login():
            return {"error": "ãƒ­ã‚°ã‚¤ãƒ³ã«å¤±æ•—ã—ã¾ã—ãŸ"}

        try:
            resp = self.session.get(f"{ADMIN_URL}/guidance/", timeout=15)
            soup = BeautifulSoup(resp.text, "html.parser")
            text = soup.get_text(separator="\n", strip=True)

            result = {
                "status": "ä¸æ˜",
                "therapists": [],
            }

            if "ä»Šã™ãã”æ¡ˆå†…å¯" in text:
                result["status"] = "â— ä»Šã™ãã”æ¡ˆå†…å¯"
            elif "ã”æ¡ˆå†…çµ‚äº†" in text:
                result["status"] = "âœ• ã”æ¡ˆå†…çµ‚äº†"
            elif "å—ä»˜ä¸­" in text:
                result["status"] = "â—‹ å—ä»˜ä¸­"

            # ã‚»ãƒ©ãƒ”ã‚¹ãƒˆæƒ…å ±ã‚’æŠ½å‡º
            lines = text.split("\n")
            for line in lines:
                line = line.strip()
                if line and len(line) < 20 and re.match(r"^[\u3040-\u309F\u30A0-\u30FF\u4E00-\u9FFF]+$", line):
                    result["therapists"].append(line)

            return result

        except Exception as e:
            logger.error(f"ã”æ¡ˆå†…çŠ¶æ³å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
            return {"error": str(e)}

    def get_schedule(self) -> dict:
        """å‡ºå‹¤è¡¨ã‚’å–å¾—"""
        if not self._ensure_login():
            return {"error": "ãƒ­ã‚°ã‚¤ãƒ³ã«å¤±æ•—ã—ã¾ã—ãŸ"}

        try:
            resp = self.session.get(f"{ADMIN_URL}/schedule/", timeout=15)
            soup = BeautifulSoup(resp.text, "html.parser")
            text = soup.get_text(separator="\n", strip=True)

            # å‡ºå‹¤è¡¨ã‹ã‚‰æœ‰ç”¨ãªæƒ…å ±ã‚’æŠ½å‡º
            schedule_lines = []
            lines = text.split("\n")
            current_date = ""

            for line in lines:
                line = line.strip()
                if not line:
                    continue

                # æ—¥ä»˜ãƒ‘ã‚¿ãƒ¼ãƒ³
                date_match = re.match(r"(\d+/\d+|\d+æœˆ\d+æ—¥)", line)
                if date_match:
                    current_date = line
                    schedule_lines.append(f"\nğŸ“… {line}")
                    continue

                # ã‚»ãƒ©ãƒ”ã‚¹ãƒˆå + æ™‚é–“
                time_match = re.search(r"\d{1,2}:\d{2}", line)
                if time_match and current_date:
                    schedule_lines.append(f"  â° {line}")

            result_text = "\n".join(schedule_lines) if schedule_lines else text[:1500]

            return {
                "schedule_text": result_text if len(result_text) <= 3000 else result_text[:3000] + "\n...",
            }

        except Exception as e:
            logger.error(f"å‡ºå‹¤è¡¨å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
            return {"error": str(e)}

    def get_reservations(self) -> dict:
        """äºˆç´„ä¸€è¦§ã‚’å–å¾—"""
        if not self._ensure_login():
            return {"error": "ãƒ­ã‚°ã‚¤ãƒ³ã«å¤±æ•—ã—ã¾ã—ãŸ"}

        try:
            resp = self.session.get(f"{ADMIN_URL}/reservation/", timeout=15)
            soup = BeautifulSoup(resp.text, "html.parser")

            reservations = []
            tables = soup.find_all("table")
            for table in tables:
                rows = table.find_all("tr")
                for row in rows:
                    cells = row.find_all(["td", "th"])
                    if cells:
                        row_text = " | ".join(c.get_text(strip=True) for c in cells)
                        if row_text.strip():
                            reservations.append(row_text)

            return {
                "reservations": reservations[:20],
                "count": len(reservations),
            }

        except Exception as e:
            logger.error(f"äºˆç´„å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
            return {"error": str(e)}

    def click_appeal(self) -> bool:
        """é›†å®¢ãƒ¯ãƒ³ã‚¯ãƒªãƒƒã‚¯ã‚¢ãƒ”ãƒ¼ãƒ«ã‚’å®Ÿè¡Œ"""
        if not self._ensure_login():
            return False

        try:
            # ã”æ¡ˆå†…çŠ¶æ³ãƒšãƒ¼ã‚¸ã«ã‚¢ã‚¯ã‚»ã‚¹
            resp = self.session.get(f"{ADMIN_URL}/guidance/", timeout=15)
            soup = BeautifulSoup(resp.text, "html.parser")

            # CSRFãƒˆãƒ¼ã‚¯ãƒ³å–å¾—
            csrf_input = soup.find("input", {"id": "csrf_footer"})
            csrf_token = csrf_input["value"] if csrf_input else self._csrf_token

            # é›†å®¢ãƒ¯ãƒ³ã‚¯ãƒªãƒƒã‚¯ã‚¢ãƒ”ãƒ¼ãƒ«ã®ãƒªãƒ³ã‚¯/ãƒœã‚¿ãƒ³ã‚’æ¢ã™
            appeal_links = soup.find_all("a", class_="send-post")
            for link in appeal_links:
                data_post = link.get("data-post", "")
                if "appeal" in data_post.lower() or "ã‚¢ãƒ”ãƒ¼ãƒ«" in link.get_text():
                    # Ajax POST ã§å®Ÿè¡Œ
                    data_form = link.get("data-form", "")
                    form = soup.find("form", {"id": f"form-{data_form}"}) if data_form else None

                    post_data = {"ctk": csrf_token}
                    if form:
                        for inp in form.find_all("input"):
                            name = inp.get("name")
                            value = inp.get("value", "")
                            if name:
                                post_data[f"str[0][name]"] = name
                                post_data[f"str[0][value]"] = value

                    resp2 = self.session.post(
                        f"{BASE_URL}/post/{data_post}",
                        data=post_data,
                        timeout=15,
                        headers={
                            "Referer": f"{ADMIN_URL}/guidance/",
                            "X-Requested-With": "XMLHttpRequest",
                        },
                    )

                    if resp2.status_code == 200:
                        logger.info("é›†å®¢ãƒ¯ãƒ³ã‚¯ãƒªãƒƒã‚¯ã‚¢ãƒ”ãƒ¼ãƒ«å®Ÿè¡ŒæˆåŠŸ")
                        return True

            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: ç›´æ¥URLã«ã‚¢ã‚¯ã‚»ã‚¹
            resp3 = self.session.get(f"{ADMIN_URL}/appeal/", timeout=15)
            if resp3.status_code == 200:
                logger.info("é›†å®¢ã‚¢ãƒ”ãƒ¼ãƒ«ãƒšãƒ¼ã‚¸ã«ã‚¢ã‚¯ã‚»ã‚¹æˆåŠŸ")
                return True

            logger.warning("ã‚¢ãƒ”ãƒ¼ãƒ«æ©Ÿèƒ½ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
            return False

        except Exception as e:
            logger.error(f"ã‚¢ãƒ”ãƒ¼ãƒ«å®Ÿè¡Œã‚¨ãƒ©ãƒ¼: {e}")
            return False

    def get_news_list(self) -> list:
        """ãƒ‹ãƒ¥ãƒ¼ã‚¹ä¸€è¦§ã‚’å–å¾—"""
        if not self._ensure_login():
            return []

        try:
            resp = self.session.get(f"{ADMIN_URL}/news/", timeout=15)
            soup = BeautifulSoup(resp.text, "html.parser")

            news_items = []
            # ãƒ†ãƒ¼ãƒ–ãƒ«ã‹ã‚‰ãƒ‹ãƒ¥ãƒ¼ã‚¹æƒ…å ±ã‚’æŠ½å‡º
            tables = soup.find_all("table")
            for table in tables:
                rows = table.find_all("tr")
                for row in rows[1:]:  # ãƒ˜ãƒƒãƒ€ãƒ¼è¡Œã‚’ã‚¹ã‚­ãƒƒãƒ—
                    cells = row.find_all("td")
                    if len(cells) >= 2:
                        news_items.append({
                            "title": cells[0].get_text(strip=True)[:50],
                            "date": cells[-1].get_text(strip=True) if len(cells) > 1 else "",
                        })

            # ãƒ†ãƒ¼ãƒ–ãƒ«ãŒãªã„å ´åˆã¯ãƒªã‚¹ãƒˆã‹ã‚‰å–å¾—
            if not news_items:
                text = soup.get_text(separator="\n", strip=True)
                lines = text.split("\n")
                for line in lines:
                    line = line.strip()
                    if line and len(line) > 5 and len(line) < 100:
                        date_match = re.search(r"\d{4}[/-]\d{1,2}[/-]\d{1,2}", line)
                        if date_match:
                            news_items.append({
                                "title": line.replace(date_match.group(), "").strip(),
                                "date": date_match.group(),
                            })

            return news_items[:10]

        except Exception as e:
            logger.error(f"ãƒ‹ãƒ¥ãƒ¼ã‚¹å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
            return []
